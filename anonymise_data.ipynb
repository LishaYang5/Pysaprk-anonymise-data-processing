{"cells":[{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/au_500.csv\"\nfile_type = \"csv\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Valid dataframe, drop missing data\ndf.na.drop()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6fad9e6f-d0ce-4363-b783-a5188eadeb34"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class='ansiout'>DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string]</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class='ansiout'>DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string, _c9: string, _c10: string]</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Rename each column\ndf = df.withColumnRenamed(\"_c0\",\"first_name\")\ndf = df.withColumnRenamed(\"_c1\",\"last_name\")\ndf = df.withColumnRenamed(\"_c2\",\"date_of_birth\")\ndf = df.withColumnRenamed(\"_c3\",\"address\")\ndf = df.withColumnRenamed(\"_c4\",\"city\")\ndf = df.withColumnRenamed(\"_c5\",\"state\")\ndf = df.withColumnRenamed(\"_c6\",\"post\")\ndf = df.withColumnRenamed(\"_c7\",\"phone1\")\ndf = df.withColumnRenamed(\"_c8\",\"phone2\")\ndf = df.withColumnRenamed(\"_c9\",\"email\")\ndf = df.withColumnRenamed(\"_c10\",\"web\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1229388b-a1dc-45c6-8a25-d1913c67771f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Anonymise first name ,last name and address\nimport pyspark.sql.functions as sf\nfrom faker import Faker\nfrom pyspark.sql import functions as F\n\nfake = Faker()\n\nfake_firstname = F.udf(fake.first_name)\ndf_fakeFirstName = df.withColumn(\"fake_FirstName\", fake_firstname())\nfake_lastname = F.udf(fake.last_name)\ndf_fakenames = df_fakeFirstName.withColumn(\"fake_LastName\",fake_lastname())\nfake_address =F.udf(fake.address)\nfake_info = df_fakenames.withColumn(\"fake_address\",fake_address())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a2360825-41ab-48b4-bcdb-4962aa5357a1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Get anonymised data for first name, last name and address\nanonymised_data = fake_info.select(\"fake_FirstName\",\"fake_LastName\",\"fake_address\",\"date_of_birth\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ad4d5d4a-9f1a-426c-b01b-a50683f398d9"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"anonymise data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":4467286779690775}},"nbformat":4,"nbformat_minor":0}
